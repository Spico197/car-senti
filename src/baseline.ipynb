{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, classification_report\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv('./../data/train/train.csv')\n",
    "    test = pd.read_csv('./../data/test_public/test_public.csv')\n",
    "    train = train.sample(frac=1) # ?\n",
    "    train = train.reset_index(drop=True)\n",
    "    \n",
    "    data = pd.concat([train, test])\n",
    "\n",
    "    lbe = LabelEncoder()\n",
    "    lbe.fit(train['subject'])\n",
    "    nb_classes = len(list(lbe.classes_))\n",
    "\n",
    "    pkl.dump(lbe, open('label_encoder.pkl', 'wb'))\n",
    "    subject = lbe.transform(train['subject'])\n",
    "\n",
    "    y = []\n",
    "    for i in train['sentiment_value'].values:\n",
    "        y.append(i)\n",
    "    y1 = []\n",
    "    for i in subject:\n",
    "        y1.append(i)\n",
    "    print(np.array(y).reshape(-1, 1)[:,0])\n",
    "    return data, train.shape[0], np.array(y).reshape(-1, 1)[:, 0], test['content_id'], np.array(y1).reshape(-1, 1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(data):\n",
    "#     jieba.load_userdict('./../data/word_dict.txt')\n",
    "    words = jieba.cut(data)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process():\n",
    "    data,nrw_train,y,test_id,y1 = get_data()\n",
    "    data['cut_comment'] = data['content'].map(processing_data)\n",
    "    print('TfidfVectorizer')\n",
    "    tf = TfidfVectorizer(ngram_range=(1,2),analyzer='char')\n",
    "    discuss_tf = tf.fit_transform(data['cut_comment'])\n",
    "    print('HashingVectorizer')\n",
    "    ha = HashingVectorizer(ngram_range=(1,1),lowercase=False)\n",
    "    discuss_ha = ha.fit_transform(data['cut_comment'])\n",
    "    data = hstack((discuss_tf,discuss_ha)).tocsr()\n",
    "    return data[:nrw_train],data[nrw_train:],y,test_id,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 -1 ...,  0  1 -1]\n",
      "TfidfVectorizer\n",
      "HashingVectorizer\n"
     ]
    }
   ],
   "source": [
    "X = pkl.load(open('X.pkl', 'rb'))\n",
    "test = pkl.load(open('test.pkl', 'rb'))\n",
    "y = pkl.load(open('y.pkl', 'rb'))\n",
    "test_id = pkl.load(open('test_id.pkl', 'rb'))\n",
    "y1 = pkl.load(open('y1.pkl', 'rb'))\n",
    "# X,test,y,test_id,y1= pre_process()\n",
    "# pkl.dump(X, open('X.pkl', 'wb'))\n",
    "# pkl.dump(test, open('test.pkl', 'wb'))\n",
    "# pkl.dump(y, open('y.pkl', 'wb'))\n",
    "# pkl.dump(test_id, open('test_id.pkl', 'wb'))\n",
    "# pkl.dump(y1, open('y1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "kf = StratifiedKFold(n_splits=N, random_state=2018).split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=0.5)\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter= 5, random_state=42)\n",
    "# clf = svm.SVC(C=1)\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier()\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oofp = np.zeros_like(y, dtype='float64')\n",
    "y_train_oofp1 = np.zeros_like(y, dtype='float64')\n",
    "\n",
    "y_test_oofp = np.zeros((test.shape[0], N))\n",
    "y_test_oofp_1 = np.zeros((test.shape[0], N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_avg_f1(y_true, y_pred):\n",
    "    return metrics.f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.704819\n",
      "i =  1\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.691457\n",
      "i =  2\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.703518\n",
      "i =  3\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.701508\n",
      "i =  4\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.696482\n",
      "i =  5\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.713568\n",
      "i =  6\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.705231\n",
      "i =  7\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.716298\n",
      "i =  8\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.712274\n",
      "i =  9\n",
      "--------------------trainning--------------------\n",
      "--------------------predicting--------------------\n",
      "sentiment_value_f1:0.714286\n",
      "0.705944047161\n",
      "0.707352897226\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "vcc = 0\n",
    "for i ,(train_fold,test_fold) in enumerate(kf):\n",
    "    print(\"i = \", i)\n",
    "    print('-'*20 + \"trainning\" + '-'*20)\n",
    "    X_train, X_validate, label_train, label_validate,  label_1_train, label_1_validate,= X[train_fold, :], X[test_fold, :], y[train_fold], y[test_fold], y1[train_fold], y1[test_fold]\n",
    "    clf.fit(X_train, label_train)\n",
    "    print('-'*20 + \"predicting\" + '-'*20)\n",
    "    val_ = clf.predict(X_validate)\n",
    "    y_train_oofp[test_fold] = val_\n",
    "    print('sentiment_value_f1:%f' % micro_avg_f1(label_validate, val_))\n",
    "    acc += micro_avg_f1(label_validate, val_)\n",
    "    result = clf.predict(test)\n",
    "    y_test_oofp[:, i] = result\n",
    "    clf.fit(X_train, label_1_train)\n",
    "    val_1 = clf.predict(X_validate)\n",
    "    y_train_oofp1[test_fold] = val_\n",
    "    vcc += micro_avg_f1(label_1_validate, val_1)\n",
    "    result = clf.predict(test)\n",
    "    y_test_oofp_1[:, i] = result\n",
    "print(acc/N)\n",
    "print(vcc/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = pkl.load(open('label_encoder.pkl','rb'))\n",
    "res_2 = []\n",
    "for i in range(y_test_oofp_1.shape[0]):\n",
    "    tmp = []\n",
    "    for j in range(N):\n",
    "        tmp.append(int(y_test_oofp_1[i][j]))\n",
    "    word_counts = Counter(tmp)\n",
    "    yes = word_counts.most_common(1)\n",
    "    res_2.append(lbl.inverse_transform(yes[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(y_test_oofp.shape[0]):\n",
    "    tmp = []\n",
    "    for j in range(N):\n",
    "        tmp.append(y_test_oofp[i][j])\n",
    "    res.append(max(set(tmp), key=tmp.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2364\n"
     ]
    }
   ],
   "source": [
    "print(len(res))\n",
    "result = pd.DataFrame()\n",
    "result['content_id'] = list(test_id)\n",
    "\n",
    "result['subject'] = list(res_2)\n",
    "result['subject'] = result['subject']\n",
    "\n",
    "result['sentiment_value'] = list(res)\n",
    "result['sentiment_value'] = result['sentiment_value'].astype(int)\n",
    "\n",
    "result['sentiment_word'] = ''\n",
    "result.to_csv('submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
